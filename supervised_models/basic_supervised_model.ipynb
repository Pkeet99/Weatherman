{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Station Supervised models\n",
    "\n",
    "## General Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import altair as alt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Small dataset (2022 only) \n",
    "### Get cleaned data from pickle file "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.realpath(os.path.join(os.getcwd(), '..'))\n",
    "cln_pkl_loc = os.path.join(ROOT_DIR, 'data_cleaning','cleanweathersmall.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_pickle(cln_pkl_loc)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic data cleaning to build necessary features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pivoted_df = df.pivot(index='time', columns='station', values=['temp', 'dwpt','rhum','prcp','wdir','wspd','pres'])\n",
    "pivoted_df.columns = ['_'.join(col) for col in pivoted_df.columns.values]\n",
    "pivoted_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our target is Ann Arbor which is station __\"KARB0\"__, so pulling those features out. And we want to predict the weather 24 hours in the future, so need to duplicate and shift the features while doing some more basic cleaning "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ann_arbor_cols = [col for col in pivoted_df.columns if \"KARB0\" in col]\n",
    "ann_arbor_df = pivoted_df[ann_arbor_cols].copy()\n",
    "for col in ann_arbor_df.columns:\n",
    "    ann_arbor_df[f'24 hr~{col}'] = ann_arbor_df[col].shift(-24)\n",
    "ann_arbor_df = ann_arbor_df.rename_axis(None, axis = 0)\n",
    "ann_arbor_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We need to merge the new features with the main dataframe so we have not only the Ann Arbor measurements, but also all measurements from surrounding stations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_df = pd.merge(pivoted_df,ann_arbor_df, left_index=True, right_index=True)\n",
    "pred_df = pred_df[pred_df['24 hr~temp_KARB0'].notna()]\n",
    "print(pred_df.shape)\n",
    "s=pred_df.isna().sum(axis=0).sort_values(ascending=False)\n",
    "pd.cut(s, 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### There are a lot of features with excessive amounts of null values to get rid of. Dropping any with more than 500 missing values still leaves a sufficient number of features for predicting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for col in pred_df.columns:\n",
    "    num = pred_df[col].isna().sum()\n",
    "    if num > 500:\n",
    "        # print(f\"{col} has {num} missing values\")\n",
    "        to_drop.append(col)\n",
    "pred_df.drop(columns=to_drop,inplace=True)\n",
    "pred_df.dropna(inplace=True)\n",
    "pred_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now our target will be the '-24hr~temp_KARB0' column, and our features to use in our first prediction model will be all of the measurements at every surrounding station 24 hours prior to our target.\n",
    "This cell will run 5 fold cross-validate on our 3 chosen regression models (Extra Trees Regressor, Lasso Regressor, and Tweedie Regressor). This will show how the average accuracy scores compare across these models on this data set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "X_cols = [col for col in pred_df.columns if \"~\" not in col]\n",
    "X = pred_df[X_cols]\n",
    "y = pred_df['24 hr~temp_KARB0']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=696)\n",
    "xt_reg = ExtraTreesRegressor(random_state=696,n_jobs=-1)\n",
    "lasso_reg = linear_model.Lasso(alpha=0.1,max_iter=1500)\n",
    "tw_reg = linear_model.TweedieRegressor(max_iter=250)\n",
    "dummy_reg = DummyRegressor(strategy=\"median\")\n",
    "models = {'Extra Trees Regressor':xt_reg,\n",
    "          'Lasso Regressor':lasso_reg,\n",
    "          'Tweedie Regressor':tw_reg,\n",
    "          'Dummy Regressor':dummy_reg}\n",
    "for key, value in models.items():\n",
    "    value = make_pipeline(StandardScaler(), value)\n",
    "    cv_results = cross_validate(value, X_train, y_train, cv=5,n_jobs=-1)\n",
    "    print(key)\n",
    "    print(\"Mean accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].mean(),3), end=\"\")\n",
    "    print(\", best accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].max(),3), end=\"\")\n",
    "    print(\", with std dev of: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].std(),3))\n",
    "    print(\"Mean training time: \", end=\"\")\n",
    "    print(round(cv_results['score_time'].mean(),3))\n",
    "    print(f\"Score on hold out set: {round(value.fit(X_train, y_train).score(X_test, y_test),3)}\")\n",
    "    print(\"**************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examine the feature importances in the best performing model (Extra Trees Regressor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame([X.columns, xt_reg.feature_importances_]).transpose()\n",
    "feature_importance_df.columns = ['Feature', 'Importance']\n",
    "feature_importance_df.sort_values('Importance',ascending=False,inplace=True)\n",
    "feature_importance_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(feature_importance_df[:5]).mark_bar().encode(\n",
    "    x=alt.X('Importance:Q', axis=alt.Axis(format=\"%\", tickSize=0, labelFontSize=12)),\n",
    "    y=alt.Y(\n",
    "        'Feature:N', sort=list(feature_importance_df[:5].Feature), title=\"\",\n",
    "        axis=alt.Axis(tickSize=0, labelFontSize=12, labelPadding=10)),\n",
    ").properties(\n",
    "    height=200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameter tuning the Extra Trees Regressor\n",
    "5 fold cross validate looking for the optimized estimators, depth, sample split, and sample leaf parameters. Evaluating the 'best' based on the mean squared error achieved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "space  = [Integer(100,200, name='n_estimators'),\n",
    "          Integer(1, 50, name='max_depth'),\n",
    "          Integer(2, 100, name='min_samples_split'),\n",
    "          Integer(1, 100, name='min_samples_leaf')]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    xt_reg.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(xt_reg, X_train, y_train, cv=5, n_jobs=-1))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=15, random_state=696)\n",
    "\n",
    "print(f\"Best score: {res_gp.fun}\")\n",
    "print(\"Best parameters:\")\n",
    "print(f\" - n-estimators= {res_gp.x[0]}\")\n",
    "print(f\" - max_depth= {res_gp.x[1]}\")\n",
    "print(f\" - min_samples_split= {res_gp.x[2]}\")\n",
    "print(f\" - min_samples_leaf=  {res_gp.x[3]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the convergence for the above hypertuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_convergence(res_gp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final re-run of the Extra Trees Model with the tuned hyperparameters\n",
    "There is a very minor improvement on the accuracy score, with some pretty significant increases in training time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xt_reg = ExtraTreesRegressor(n_estimators=res_gp.x[0], \n",
    "                             max_depth=res_gp.x[1], \n",
    "                             min_samples_split=res_gp.x[2], \n",
    "                             min_samples_leaf=res_gp.x[3], \n",
    "                             random_state=696,n_jobs=-1\n",
    "                            )\n",
    "models = {'Extra Trees Regressor':xt_reg}\n",
    "for key, value in models.items():\n",
    "    cv_results = cross_validate(value, X_train, y_train, cv=5,n_jobs=-1)\n",
    "    print(key)\n",
    "    print(\"Mean accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].mean(),3), end=\"\")\n",
    "    print(\", best accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].max(),3), end=\"\")\n",
    "    print(\", with std dev of: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].std(),3))\n",
    "    print(\"Mean training time: \", end=\"\")\n",
    "    print(round(cv_results['score_time'].mean(),3))\n",
    "    print(f\"Score on hold out set: {round(value.fit(X_train, y_train).score(X_test, y_test),3)}\")\n",
    "    print(\"**************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full US dataset (2019-Sept 2022)\n",
    "### Get cleaned data from pickle file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.realpath(os.path.join(os.getcwd(), '..'))\n",
    "cln_pkl_loc = os.path.join(ROOT_DIR, 'data_cleaning','cleanweather.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60577995 entries, 0 to 60577994\n",
      "Data columns (total 9 columns):\n",
      " #   Column   Dtype         \n",
      "---  ------   -----         \n",
      " 0   station  object        \n",
      " 1   time     datetime64[ns]\n",
      " 2   temp     float64       \n",
      " 3   dwpt     float64       \n",
      " 4   rhum     float64       \n",
      " 5   prcp     float64       \n",
      " 6   wdir     float64       \n",
      " 7   wspd     float64       \n",
      " 8   pres     float64       \n",
      "dtypes: datetime64[ns](1), float64(7), object(1)\n",
      "memory usage: 4.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(cln_pkl_loc)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic data cleaning to build necessary features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                     temp_04AEH  temp_0CC8G  temp_0CNUO  temp_0CO7B  \\\ntime                                                                  \n2019-01-01 00:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 01:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 02:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 03:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 04:00:00         NaN         NaN         NaN         NaN   \n...                         ...         ...         ...         ...   \n2022-09-02 19:00:00        23.9        22.0        20.5        27.8   \n2022-09-02 20:00:00        24.5        22.0        20.8        28.1   \n2022-09-02 21:00:00        24.0        22.0        20.7        28.2   \n2022-09-02 22:00:00        24.0        21.5        20.4        27.7   \n2022-09-02 23:00:00        21.4        20.1        19.4        26.3   \n\n                     temp_0FV1F  temp_0FV2W  temp_0JM7R  temp_0JPFS  \\\ntime                                                                  \n2019-01-01 00:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 01:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 02:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 03:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 04:00:00         NaN         NaN         NaN         NaN   \n...                         ...         ...         ...         ...   \n2022-09-02 19:00:00        26.5         NaN        27.7        19.9   \n2022-09-02 20:00:00        26.2         NaN        28.5        19.9   \n2022-09-02 21:00:00        25.9         NaN        28.0        19.6   \n2022-09-02 22:00:00        25.4         NaN        27.5        18.6   \n2022-09-02 23:00:00        23.9         NaN        26.1        16.5   \n\n                     temp_0NNEW  temp_0RJDR  ...  pres_Z8Y0M  pres_ZFS01  \\\ntime                                         ...                           \n2019-01-01 00:00:00         NaN         NaN  ...         NaN         NaN   \n2019-01-01 01:00:00         NaN         NaN  ...         NaN         NaN   \n2019-01-01 02:00:00         NaN         NaN  ...         NaN         NaN   \n2019-01-01 03:00:00         NaN         NaN  ...         NaN         NaN   \n2019-01-01 04:00:00         NaN         NaN  ...         NaN         NaN   \n...                         ...         ...  ...         ...         ...   \n2022-09-02 19:00:00        28.8        32.8  ...      1023.0      1022.2   \n2022-09-02 20:00:00        28.4        32.8  ...      1023.0      1021.6   \n2022-09-02 21:00:00        28.2        31.2  ...      1022.0      1021.5   \n2022-09-02 22:00:00        27.6        32.8  ...      1022.0      1021.5   \n2022-09-02 23:00:00        26.8        30.7  ...      1014.1      1021.5   \n\n                     pres_ZFZUV  pres_ZJ8AR  pres_ZNWZW  pres_ZRBBD  \\\ntime                                                                  \n2019-01-01 00:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 01:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 02:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 03:00:00         NaN         NaN         NaN         NaN   \n2019-01-01 04:00:00         NaN         NaN         NaN         NaN   \n...                         ...         ...         ...         ...   \n2022-09-02 19:00:00      1014.8      1013.6      1012.2      1016.5   \n2022-09-02 20:00:00      1015.5      1014.7      1012.3      1015.8   \n2022-09-02 21:00:00      1016.1      1014.3      1014.0      1014.4   \n2022-09-02 22:00:00      1016.4      1014.8      1012.7      1013.8   \n2022-09-02 23:00:00      1017.6      1014.8      1011.1      1013.5   \n\n                     pres_ZUQJS  pres_ZWC6W  pres_ZYC17  pres_ZYITU  \ntime                                                                 \n2019-01-01 00:00:00         NaN         NaN         NaN         NaN  \n2019-01-01 01:00:00         NaN         NaN         NaN         NaN  \n2019-01-01 02:00:00         NaN         NaN         NaN         NaN  \n2019-01-01 03:00:00         NaN         NaN         NaN         NaN  \n2019-01-01 04:00:00         NaN         NaN         NaN         NaN  \n...                         ...         ...         ...         ...  \n2022-09-02 19:00:00      1013.0      1003.7      1022.9      1018.8  \n2022-09-02 20:00:00      1012.9      1003.3      1022.5      1018.8  \n2022-09-02 21:00:00      1012.8      1003.1      1022.5      1018.0  \n2022-09-02 22:00:00      1013.2      1003.9      1022.4      1017.7  \n2022-09-02 23:00:00      1012.3      1004.6      1022.2      1017.7  \n\n[32184 rows x 16275 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temp_04AEH</th>\n      <th>temp_0CC8G</th>\n      <th>temp_0CNUO</th>\n      <th>temp_0CO7B</th>\n      <th>temp_0FV1F</th>\n      <th>temp_0FV2W</th>\n      <th>temp_0JM7R</th>\n      <th>temp_0JPFS</th>\n      <th>temp_0NNEW</th>\n      <th>temp_0RJDR</th>\n      <th>...</th>\n      <th>pres_Z8Y0M</th>\n      <th>pres_ZFS01</th>\n      <th>pres_ZFZUV</th>\n      <th>pres_ZJ8AR</th>\n      <th>pres_ZNWZW</th>\n      <th>pres_ZRBBD</th>\n      <th>pres_ZUQJS</th>\n      <th>pres_ZWC6W</th>\n      <th>pres_ZYC17</th>\n      <th>pres_ZYITU</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-01-01 00:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 01:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 02:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 03:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 04:00:00</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-09-02 19:00:00</th>\n      <td>23.9</td>\n      <td>22.0</td>\n      <td>20.5</td>\n      <td>27.8</td>\n      <td>26.5</td>\n      <td>NaN</td>\n      <td>27.7</td>\n      <td>19.9</td>\n      <td>28.8</td>\n      <td>32.8</td>\n      <td>...</td>\n      <td>1023.0</td>\n      <td>1022.2</td>\n      <td>1014.8</td>\n      <td>1013.6</td>\n      <td>1012.2</td>\n      <td>1016.5</td>\n      <td>1013.0</td>\n      <td>1003.7</td>\n      <td>1022.9</td>\n      <td>1018.8</td>\n    </tr>\n    <tr>\n      <th>2022-09-02 20:00:00</th>\n      <td>24.5</td>\n      <td>22.0</td>\n      <td>20.8</td>\n      <td>28.1</td>\n      <td>26.2</td>\n      <td>NaN</td>\n      <td>28.5</td>\n      <td>19.9</td>\n      <td>28.4</td>\n      <td>32.8</td>\n      <td>...</td>\n      <td>1023.0</td>\n      <td>1021.6</td>\n      <td>1015.5</td>\n      <td>1014.7</td>\n      <td>1012.3</td>\n      <td>1015.8</td>\n      <td>1012.9</td>\n      <td>1003.3</td>\n      <td>1022.5</td>\n      <td>1018.8</td>\n    </tr>\n    <tr>\n      <th>2022-09-02 21:00:00</th>\n      <td>24.0</td>\n      <td>22.0</td>\n      <td>20.7</td>\n      <td>28.2</td>\n      <td>25.9</td>\n      <td>NaN</td>\n      <td>28.0</td>\n      <td>19.6</td>\n      <td>28.2</td>\n      <td>31.2</td>\n      <td>...</td>\n      <td>1022.0</td>\n      <td>1021.5</td>\n      <td>1016.1</td>\n      <td>1014.3</td>\n      <td>1014.0</td>\n      <td>1014.4</td>\n      <td>1012.8</td>\n      <td>1003.1</td>\n      <td>1022.5</td>\n      <td>1018.0</td>\n    </tr>\n    <tr>\n      <th>2022-09-02 22:00:00</th>\n      <td>24.0</td>\n      <td>21.5</td>\n      <td>20.4</td>\n      <td>27.7</td>\n      <td>25.4</td>\n      <td>NaN</td>\n      <td>27.5</td>\n      <td>18.6</td>\n      <td>27.6</td>\n      <td>32.8</td>\n      <td>...</td>\n      <td>1022.0</td>\n      <td>1021.5</td>\n      <td>1016.4</td>\n      <td>1014.8</td>\n      <td>1012.7</td>\n      <td>1013.8</td>\n      <td>1013.2</td>\n      <td>1003.9</td>\n      <td>1022.4</td>\n      <td>1017.7</td>\n    </tr>\n    <tr>\n      <th>2022-09-02 23:00:00</th>\n      <td>21.4</td>\n      <td>20.1</td>\n      <td>19.4</td>\n      <td>26.3</td>\n      <td>23.9</td>\n      <td>NaN</td>\n      <td>26.1</td>\n      <td>16.5</td>\n      <td>26.8</td>\n      <td>30.7</td>\n      <td>...</td>\n      <td>1014.1</td>\n      <td>1021.5</td>\n      <td>1017.6</td>\n      <td>1014.8</td>\n      <td>1011.1</td>\n      <td>1013.5</td>\n      <td>1012.3</td>\n      <td>1004.6</td>\n      <td>1022.2</td>\n      <td>1017.7</td>\n    </tr>\n  </tbody>\n</table>\n<p>32184 rows × 16275 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_df = df.pivot(index='time', columns='station', values=['temp', 'dwpt','rhum','prcp','wdir','wspd','pres'])\n",
    "pivoted_df.columns = ['_'.join(col) for col in pivoted_df.columns.values]\n",
    "pivoted_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Our target is Ann Arbor which is station __\"KARB0\"__, so pulling those features out. And we want to predict the weather 24 hours in the future, so need to duplicate and shift the features while doing some more basic cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     temp_KARB0  dwpt_KARB0  rhum_KARB0  prcp_KARB0  \\\n2019-01-01 00:00:00         3.9         3.9       100.0         NaN   \n2019-01-01 01:00:00         4.4         4.0        97.0         0.5   \n2019-01-01 02:00:00         4.4         4.4       100.0         1.5   \n2019-01-01 03:00:00         7.8         7.2        96.0         NaN   \n2019-01-01 04:00:00         6.1         5.7        97.0         NaN   \n\n                     wdir_KARB0  wspd_KARB0  pres_KARB0  24 hr~temp_KARB0  \\\n2019-01-01 00:00:00        60.0         9.4         NaN              -1.1   \n2019-01-01 01:00:00         NaN         0.0       996.8              -1.1   \n2019-01-01 02:00:00         NaN         0.0       996.9              -1.7   \n2019-01-01 03:00:00       220.0        20.5       997.3              -1.7   \n2019-01-01 04:00:00       260.0        29.5       999.8              -2.2   \n\n                     24 hr~dwpt_KARB0  24 hr~rhum_KARB0  24 hr~prcp_KARB0  \\\n2019-01-01 00:00:00              -2.2              92.0               0.0   \n2019-01-01 01:00:00              -2.2              92.0               0.0   \n2019-01-01 02:00:00              -2.3              96.0               0.0   \n2019-01-01 03:00:00              -2.8              92.0               0.0   \n2019-01-01 04:00:00              -3.3              92.0               0.0   \n\n                     24 hr~wdir_KARB0  24 hr~wspd_KARB0  24 hr~pres_KARB0  \n2019-01-01 00:00:00             350.0              11.2            1025.4  \n2019-01-01 01:00:00             360.0               5.4            1026.1  \n2019-01-01 02:00:00             350.0               7.6            1026.0  \n2019-01-01 03:00:00              10.0               7.6            1025.8  \n2019-01-01 04:00:00              50.0               5.4            1025.7  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temp_KARB0</th>\n      <th>dwpt_KARB0</th>\n      <th>rhum_KARB0</th>\n      <th>prcp_KARB0</th>\n      <th>wdir_KARB0</th>\n      <th>wspd_KARB0</th>\n      <th>pres_KARB0</th>\n      <th>24 hr~temp_KARB0</th>\n      <th>24 hr~dwpt_KARB0</th>\n      <th>24 hr~rhum_KARB0</th>\n      <th>24 hr~prcp_KARB0</th>\n      <th>24 hr~wdir_KARB0</th>\n      <th>24 hr~wspd_KARB0</th>\n      <th>24 hr~pres_KARB0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-01-01 00:00:00</th>\n      <td>3.9</td>\n      <td>3.9</td>\n      <td>100.0</td>\n      <td>NaN</td>\n      <td>60.0</td>\n      <td>9.4</td>\n      <td>NaN</td>\n      <td>-1.1</td>\n      <td>-2.2</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>350.0</td>\n      <td>11.2</td>\n      <td>1025.4</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 01:00:00</th>\n      <td>4.4</td>\n      <td>4.0</td>\n      <td>97.0</td>\n      <td>0.5</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>996.8</td>\n      <td>-1.1</td>\n      <td>-2.2</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>360.0</td>\n      <td>5.4</td>\n      <td>1026.1</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 02:00:00</th>\n      <td>4.4</td>\n      <td>4.4</td>\n      <td>100.0</td>\n      <td>1.5</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>996.9</td>\n      <td>-1.7</td>\n      <td>-2.3</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>350.0</td>\n      <td>7.6</td>\n      <td>1026.0</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 03:00:00</th>\n      <td>7.8</td>\n      <td>7.2</td>\n      <td>96.0</td>\n      <td>NaN</td>\n      <td>220.0</td>\n      <td>20.5</td>\n      <td>997.3</td>\n      <td>-1.7</td>\n      <td>-2.8</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>7.6</td>\n      <td>1025.8</td>\n    </tr>\n    <tr>\n      <th>2019-01-01 04:00:00</th>\n      <td>6.1</td>\n      <td>5.7</td>\n      <td>97.0</td>\n      <td>NaN</td>\n      <td>260.0</td>\n      <td>29.5</td>\n      <td>999.8</td>\n      <td>-2.2</td>\n      <td>-3.3</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>50.0</td>\n      <td>5.4</td>\n      <td>1025.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_arbor_cols = [col for col in pivoted_df.columns if \"KARB0\" in col]\n",
    "ann_arbor_df = pivoted_df[ann_arbor_cols].copy()\n",
    "for col in ann_arbor_df.columns:\n",
    "    ann_arbor_df[f'24 hr~{col}'] = ann_arbor_df[col].shift(-24)\n",
    "ann_arbor_df = ann_arbor_df.rename_axis(None, axis = 0)\n",
    "ann_arbor_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We need to merge the new features with the main dataframe so we have not only the Ann Arbor measurements, but also all measurements from surrounding stations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31771, 16289)\n"
     ]
    },
    {
     "data": {
      "text/plain": "prcp_KMHN0    (28557.9, 31731.0]\nwspd_KMHN0    (28557.9, 31731.0]\npres_KMHN0    (28557.9, 31731.0]\nwdir_KMHN0    (28557.9, 31731.0]\nrhum_KMHN0    (28557.9, 31731.0]\n                     ...        \nrhum_72565     (-31.731, 3173.1]\ndwpt_72334     (-31.731, 3173.1]\ntemp_72659     (-31.731, 3173.1]\ndwpt_72340     (-31.731, 3173.1]\ndwpt_72568     (-31.731, 3173.1]\nLength: 16289, dtype: category\nCategories (10, interval[float64, right]): [(-31.731, 3173.1] < (3173.1, 6346.2] < (6346.2, 9519.3] < (9519.3, 12692.4] ... (19038.6, 22211.7] < (22211.7, 25384.8] < (25384.8, 28557.9] < (28557.9, 31731.0]]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.merge(pivoted_df,ann_arbor_df, left_index=True, right_index=True)\n",
    "pred_df = pred_df[pred_df['24 hr~temp_KARB0'].notna()]\n",
    "print(pred_df.shape)\n",
    "s=pred_df.isna().sum(axis=0).sort_values(ascending=False)\n",
    "pd.cut(s, 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### There are a lot of features with excessive amounts of null values to get rid of. Dropping any with more than 500 missing values still leaves a sufficient number of features for predicting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(3654, 8690)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = []\n",
    "for col in pred_df.columns:\n",
    "    num = pred_df[col].isna().sum()\n",
    "    if num > 3200:\n",
    "        # print(f\"{col} has {num} missing values\")\n",
    "        to_drop.append(col)\n",
    "pred_df.drop(columns=to_drop,inplace=True)\n",
    "pred_df.dropna(inplace=True)\n",
    "pred_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now our target will be the '-24hr~temp_KARB0' column, and our features to use in our first prediction model will be all of the measurements at every surrounding station 24 hours prior to our target.\n",
    "This cell will run 5 fold cross-validate on our 3 chosen regression models (Extra Trees Regressor, Lasso Regressor, and Tweedie Regressor). This will show how the average accuracy scores compare across these models on this data set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "X_cols = [col for col in pred_df.columns if \"~\" not in col]\n",
    "X = pred_df[X_cols]\n",
    "y = pred_df['24 hr~temp_KARB0']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=696)\n",
    "xt_reg = ExtraTreesRegressor(random_state=696,n_jobs=-1)\n",
    "lasso_reg = linear_model.Lasso(alpha=0.1,max_iter=1500)\n",
    "tw_reg = linear_model.TweedieRegressor(max_iter=250)\n",
    "dummy_reg = DummyRegressor(strategy=\"median\")\n",
    "models = {'Extra Trees Regressor':xt_reg,\n",
    "          'Lasso Regressor':lasso_reg,\n",
    "          'Tweedie Regressor':tw_reg,\n",
    "          'Dummy Regressor':dummy_reg}\n",
    "for key, value in models.items():\n",
    "    # value = make_pipeline(StandardScaler(), value)\n",
    "    cv_results = cross_validate(value, X_train, y_train, cv=5,n_jobs=-1)\n",
    "    print(key)\n",
    "    print(\"Mean accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].mean(),3), end=\"\")\n",
    "    print(\", best accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].max(),3), end=\"\")\n",
    "    print(\", with std dev of: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].std(),3))\n",
    "    print(\"Mean training time: \", end=\"\")\n",
    "    print(round(cv_results['score_time'].mean(),3))\n",
    "    print(f\"Score on hold out set: {round(value.fit(X_train, y_train).score(X_test, y_test),3)}\")\n",
    "    print(\"**************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Examine the feature importances in the best performing model (Extra Trees Regressor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame([X.columns, xt_reg.feature_importances_]).transpose()\n",
    "feature_importance_df.columns = ['Feature', 'Importance']\n",
    "feature_importance_df.sort_values('Importance',ascending=False,inplace=True)\n",
    "feature_importance_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(feature_importance_df[:5]).mark_bar().encode(\n",
    "    x=alt.X('Importance:Q', axis=alt.Axis(format=\"%\", tickSize=0, labelFontSize=12)),\n",
    "    y=alt.Y(\n",
    "        'Feature:N', sort=list(feature_importance_df[:5].Feature), title=\"\",\n",
    "        axis=alt.Axis(tickSize=0, labelFontSize=12, labelPadding=10)),\n",
    ").properties(\n",
    "    height=200\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameter tuning the Extra Trees Regressor\n",
    "5 fold cross validate looking for the optimized estimators, depth, sample split, and sample leaf parameters. Evaluating the 'best' based on the mean squared error achieved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "space  = [Integer(100,200, name='n_estimators'),\n",
    "          Integer(1, 50, name='max_depth'),\n",
    "          Integer(2, 100, name='min_samples_split'),\n",
    "          Integer(1, 100, name='min_samples_leaf')]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    xt_reg.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(xt_reg, X_train, y_train, cv=5, n_jobs=-1))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=15, random_state=696)\n",
    "\n",
    "print(f\"Best score: {res_gp.fun}\")\n",
    "print(\"Best parameters:\")\n",
    "print(f\" - n-estimators= {res_gp.x[0]}\")\n",
    "print(f\" - max_depth= {res_gp.x[1]}\")\n",
    "print(f\" - min_samples_split= {res_gp.x[2]}\")\n",
    "print(f\" - min_samples_leaf=  {res_gp.x[3]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the convergence for the above hypertuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_convergence(res_gp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final re-run of the Extra Trees Model with the tuned hyperparameters\n",
    "There is a very minor improvement on the accuracy score, with some pretty significant increases in training time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xt_reg = ExtraTreesRegressor(n_estimators=res_gp.x[0],\n",
    "                             max_depth=res_gp.x[1],\n",
    "                             min_samples_split=res_gp.x[2],\n",
    "                             min_samples_leaf=res_gp.x[3],\n",
    "                             random_state=696,n_jobs=-1\n",
    "                            )\n",
    "models = {'Extra Trees Regressor':xt_reg}\n",
    "for key, value in models.items():\n",
    "    cv_results = cross_validate(value, X_train, y_train, cv=5,n_jobs=-1)\n",
    "    print(key)\n",
    "    print(\"Mean accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].mean(),3), end=\"\")\n",
    "    print(\", best accuracy score: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].max(),3), end=\"\")\n",
    "    print(\", with std dev of: \", end=\"\")\n",
    "    print(round(cv_results['test_score'].std(),3))\n",
    "    print(\"Mean training time: \", end=\"\")\n",
    "    print(round(cv_results['score_time'].mean(),3))\n",
    "    print(f\"Score on hold out set: {round(value.fit(X_train, y_train).score(X_test, y_test),3)}\")\n",
    "    print(\"**************\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}